<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.43">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Krzysztof Wojdalski">
<meta name="dcterms.date" content="2025-04-24">

<title>Reinforcement Learning Portfolio Optimization for FX Trading</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="masters_thesis_files/libs/clipboard/clipboard.min.js"></script>
<script src="masters_thesis_files/libs/quarto-html/quarto.js"></script>
<script src="masters_thesis_files/libs/quarto-html/popper.min.js"></script>
<script src="masters_thesis_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="masters_thesis_files/libs/quarto-html/anchor.min.js"></script>
<link href="masters_thesis_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="masters_thesis_files/libs/quarto-html/quarto-syntax-highlighting-d4d76bf8491c20bad77d141916dc28e1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="masters_thesis_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="masters_thesis_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="masters_thesis_files/libs/bootstrap/bootstrap-c0367b04c37547644fece4185067e4a7.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
</head><body class="fullcontent">\usepackage[utf8]{inputenc}
\usepackage{amsthm}
\usepackage{rotating}
\pagenumbering{gobble}

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>





<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Reinforcement Learning Portfolio Optimization for FX Trading</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Krzysztof Wojdalski </p>
          </div>
  </div>

    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">April 24, 2025</p>
    </div>
  </div>


  </div>



</header>


<section id="abstract" class="level1">
<h1>Abstract</h1>
<p>The work is about reinforcement learning application in trading on the FX market. The author starts with describing the FX market, analyzing market organization, participants, and changes in the last years. He tries to explain current trends and the possible directions. The next part consists of theoretical pattern for the research - description of financial models, and the AI algorithms.</p>
<p>Implementation of the RL-based approach in the third chapter, based on Q-learning, gives spurious results.</p>
</section>
<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>The conventional representation of trading floors as chaotic environments characterized by vocal trader interactions was historically accurate approximately three decades ago. However, the financial industry has since undergone substantial structural transformation.</p>
<p>The financial sector has consistently functioned as an early adopter of computational innovations. This technological integration represents a strategic imperative—competitive advantage through advanced technological implementation frequently correlates with enhanced financial performance metrics. The industry has systematically pioneered the deployment of state-of-the-art technologies, ranging from sophisticated Bloomberg terminal infrastructure in the 1990s to contemporary blockchain applications and ultra-low latency systems, all oriented toward maximizing operational efficiency<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>.</p>
<p>Trading entities, functioning as utility-maximizing economic agents, fundamentally aim to optimize market-derived profits. Multiple methodological approaches exist to achieve this objective, with varying degrees of complexity. Notably, Warren Buffett, a statistically significant outlier in investment performance metrics, continues to employ a passive buy-and-hold strategy.</p>
<p>In recent years, methodologies predicated on artificial intelligence and machine learning algorithms have demonstrated increasing significance. This phenomenon correlates with advancements in computational processing capacity, decreasing infrastructure expenditure requirements, and empirical recognition of cognitive biases in human decision-making processes <span class="citation" data-cites="Arnold2017">(<a href="#ref-Arnold2017" role="doc-biblioref"><strong>Arnold2017?</strong></a>)</span>. A growing consensus within the literature suggests that algorithmic systems should supplement or potentially supersede human involvement in decision-making and execution processes <span class="citation" data-cites="Turner2015">Turner (<a href="#ref-Turner2015" role="doc-biblioref">2015</a>)</span>. The progressive automation of trading activities will likely continue its trajectory in subsequent temporal periods, contradicting the aforementioned stereotypical trading floor representation.</p>
<p>Although machine learning’s theoretical foundations date to the 1950s, its explicit implementation in trading contexts remains relatively limited. For example, in institutional foreign exchange trading, only entities with substantial capital resources and sophisticated quantitative infrastructure have developed effective machine learning trading systems <span class="citation" data-cites="Mosic2017">Mosic (<a href="#ref-Mosic2017" role="doc-biblioref">2017</a>)</span>.</p>
<p>This research investigates potential applications of machine learning—specifically reinforcement learning methodologies—in developing trading systems capable of generating statistically significant positive outcomes.</p>
<p>Currently, most systems documented in academic literature aim to maximize either absolute trading profits or risk-adjusted performance metrics. Despite numerous methodological approaches to create consistently profitable systems utilizing variables derived from financial econometrics, fundamental analysis, or machine learning algorithms, many have demonstrated suboptimal performance due to several factors, including:</p>
<ul>
<li>Frequent large drawdowns resulting in excessive performance volatility</li>
<li>Prohibitive transaction costs rendering strategies economically unfeasible</li>
<li>Excessive computational complexity, particularly problematic for high-frequency trading implementations</li>
</ul>
<p>Even when empirical research demonstrates exceptional results, upon publication, any competitive advantage tends to diminish through market efficiency mechanisms. Strategies with persistent alpha-generating capabilities must remain proprietary to maintain their effectiveness.</p>
<section id="work-structure" class="level3">
<h3 class="anchored" data-anchor-id="work-structure">Work Structure</h3>
<p>This thesis is structured to provide a comprehensive analytical framework for understanding reinforcement learning algorithms applied to trading contexts.</p>
<p>The first section provides a detailed examination of the intersection between artificial intelligence methodologies and financial markets, exploring the historical relationship between quantitative finance and computational science.</p>
<p>The second chapter presents a systematic review of selected literature from quantitative finance, examining both classical equilibrium models such as CAPM (the established paradigm in equity research) and contemporary approaches. This section evaluates the implicit advantages and limitations of various financial models, with particular emphasis on algorithmic trading methodologies employed in comparable research contexts.</p>
<p>The third section analyzes machine learning frameworks, providing a theoretical basis for why reinforcement learning may represent an optimal approach for certain trading applications. It presents a taxonomic comparison of major machine learning categories to elucidate their methodological distinctions, introduces key reinforcement learning concepts with illustrative examples, and addresses potential limitations and implementation challenges associated with these algorithms.</p>
<p>The fourth part details the experimental methodology, including research objectives, data characteristics, experimental design parameters, and empirical results. The primary objective was to develop trading agents capable of statistically outperforming established benchmarks on risk-adjusted performance measures in the foreign exchange market—agents characterized by statistical robustness, adaptive learning capability, and consistent performance metrics. This chapter presents the mathematical formulations and procedural implementations leading to the empirical results, examining each component of the trading system.</p>
<p>The implemented algorithms utilize a dynamic optimization approach. Beyond a value function based on Differential Sharpe Ratio, the system incorporates various technical indicators such as Relative Strength Index to inform algorithmic decision-making processes. The methodology incorporates transaction cost models to simulate realistic trading conditions.</p>
<p>The value function integrates multiple statistical measures, including Sharpe and Differential Sharpe Ratios, to capture both risk and return dimensions. The algorithm outputs agent actions in the discrete action space {-1,0,1}. The final section of this chapter evaluates the reinforcement learning-based trading system against two benchmark methodologies:</p>
<ul>
<li>A buy-and-hold strategy (maintaining consistent long positions in selected currency pairs)</li>
<li>Random action generation—producing stochastic values in the domain of {-1,0,1} to determine positions in underlying pairs. This benchmark excludes transaction costs, as such a strategy would incur prohibitive cumulative costs with position changes occurring in approximately two-thirds of states.</li>
</ul>
<p>The concluding section presents a comparative analysis with similar research and proposes directions for future investigation, addressing research questions such as:</p>
<ul>
<li>What additional implementations could enhance performance metrics?</li>
<li>What methodological limitations were encountered and how might they be addressed in subsequent research?</li>
</ul>
<!--!Rnw root = ../../masters\\_thesis.Rnw -->
</section>
<section id="fx-market-organization" class="level2">
<h2 class="anchored" data-anchor-id="fx-market-organization">FX Market Organization</h2>
<p>Explaining the institutional structure of FX market requires introducing formal definitions of market organization. According to Lyons <span class="citation" data-cites="Lyons2002">Lyons (<a href="#ref-Lyons2002" role="doc-biblioref">2002</a>)</span>, these are:</p>
<ul>
<li>Auction market - a participant can place a market and a limit order. The first action is aimed at buying X units at the best price. Alternatively, limit orders set a threshold, i.e.&nbsp;they are executed only if the market quotes reach a certain price. Limit orders are aggregated into an order book</li>
<li>Single dealer market - in this kind of market organization, there is just one dealer. It is obliged to quote an asset, i.e.&nbsp;to match demand and supply. Its quotations are always the best bid and the best ask. The main task is to manage the risk to make profit off his spread.</li>
<li>Multiple dealer market - it is extension of single dealer market. There is more than one dealer and they compete against each other. It might be centralized or decentralized. In the first version, all dealers are put into the same location while in the second it is not the case. When the market is decentralized, it is possible for price takers to gain profits by arbitrage transactions.</li>
</ul>
<p>The FX market is a kind of decentralized multiple-dealer market. There is no single indicator that would show the best bid and the best ask. Hence, the market transparency is low. It is especially important at tail events. It is hard to determine when the market was at a given time and findings are usually spurious.</p>
<p>The foreign exchange market is perceived as the largest and most liquid one, with a year-on-year turnover of trillion.</p>
<p>The FX market is an over the counter, global (OTC) market, i.e.&nbsp;participants can trade currencies with relatively low level of legal obstacles. The market core is built up by the biggest banks in the world. Hence, the FX organization is often referred as an inter-bank market. The participants of the FX market differ by access, spreads, impact, turnover they generate, order size, and purpose. They can be divided into five main groups:</p>
<ul>
<li>Central banks - they control money supply, interest and reserve rates and hence can have the strongest market impact. Through their set of tools, they strengthen or weaken local currency. In the developed markets, their turnover is rather small due to the fact that intervenings happen rarely. On the other hand, order size is usually bigger than for other four groups due to the effect they want to achieve.</li>
<li>Commercial banks - most of the flow in the market belongs to commercial banks. Although the environment in which FX trading occurs is highly dispersed in terms of location, over 85% of flow is generated by top 15 banks, as seen in <span class="math inline">\(\ref{table_turnover_banks}\)</span>. It can be observed even for currencies that the banks do not have real interest in. It means that in fact banks stay with flat position. Commercial banks make money on effective risk management. Essentially, it means taking flows from clients (retail/institutional) and managing risk books. Over the years, the market have changed dramatically. Even though turnovers are higher than ten years, market practitioners tend to claim that liquidity has worsen. It is mostly due to the fact that new regulation, internal and external have been introduced. Banks are required to stay with rather small positions, especially in non-G10 currencies. Their approach to risk is much more conservative than it used to be.</li>
<li>Non-bank financial institutions - their significance as market participants is on the rise. Even though, non-bank financial institutions category is very broad and entities in it are very heterogenous, the most impactful are sophisticated hedge funds focused on effective market making (such as XTX Markets).</li>
<li>Commercial companies - as price takers they are significantly worse than commercial banks due to the fact they trade bigger size and mainly hedge their main business.</li>
<li>Retail traders - their main purpose is to speculate. The conditions they receive from financial institutions are generally worse.</li>
</ul>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: right;">Rank</th>
<th style="text-align: left;">Bank</th>
<th style="text-align: left;">Market Share</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">1</td>
<td style="text-align: left;">Citi</td>
<td style="text-align: left;">16.11%</td>
</tr>
<tr class="even">
<td style="text-align: right;">2</td>
<td style="text-align: left;">Deutsche Bank</td>
<td style="text-align: left;">14.54%</td>
</tr>
<tr class="odd">
<td style="text-align: right;">3</td>
<td style="text-align: left;">Barclays</td>
<td style="text-align: left;">8.11%</td>
</tr>
<tr class="even">
<td style="text-align: right;">4</td>
<td style="text-align: left;">JPMorgan</td>
<td style="text-align: left;">7.65%</td>
</tr>
<tr class="odd">
<td style="text-align: right;">5</td>
<td style="text-align: left;">UBS</td>
<td style="text-align: left;">7.3%</td>
</tr>
<tr class="even">
<td style="text-align: right;">6</td>
<td style="text-align: left;">Bank of America Merrill Lynch</td>
<td style="text-align: left;">6.22%</td>
</tr>
<tr class="odd">
<td style="text-align: right;">7</td>
<td style="text-align: left;">HSBC</td>
<td style="text-align: left;">5.4%</td>
</tr>
<tr class="even">
<td style="text-align: right;">8</td>
<td style="text-align: left;">BNP Paribas</td>
<td style="text-align: left;">3.65%</td>
</tr>
<tr class="odd">
<td style="text-align: right;">9</td>
<td style="text-align: left;">Goldman Sachs</td>
<td style="text-align: left;">3.4%</td>
</tr>
<tr class="even">
<td style="text-align: right;">10</td>
<td style="text-align: left;">RBS</td>
<td style="text-align: left;">3.38%</td>
</tr>
<tr class="odd">
<td style="text-align: right;">11</td>
<td style="text-align: left;">Societe Generale</td>
<td style="text-align: left;">2.43%</td>
</tr>
<tr class="even">
<td style="text-align: right;">12</td>
<td style="text-align: left;">Standard Chartered</td>
<td style="text-align: left;">2.4%</td>
</tr>
<tr class="odd">
<td style="text-align: right;">13</td>
<td style="text-align: left;">Morgan Stanley</td>
<td style="text-align: left;">1.97%</td>
</tr>
<tr class="even">
<td style="text-align: right;">14</td>
<td style="text-align: left;">Credit Suisse</td>
<td style="text-align: left;">1.66%</td>
</tr>
<tr class="odd">
<td style="text-align: right;">15</td>
<td style="text-align: left;">State Street</td>
<td style="text-align: left;">1.55%</td>
</tr>
</tbody>
</table>
<p>In the last years, there have been observed shifting towards eFX (electronic trading of FX). Commercial banks, as mentioned in the previous subsection, are subject to new regulations. Therefore, right now they are more concerned about increasing their turnover than benefiting off speculation, e.g.&nbsp;trading based on macro research. eFX helps in this goal. It requires more technology while a number of traditional dealers is effectively reduced. The activity require quantitative analysts, “quants”, who can manage pricing engines in order to maximize profit while staying within risk constraints. Over the last 4 years, eFX gained 13 percent point and in 2015 for the first time surpassed voice trading, with 53.2% of client flow share <span class="citation" data-cites="JeffPatterson2015">(<a href="#ref-JeffPatterson2015" role="doc-biblioref"><strong>JeffPatterson2015?</strong></a>)</span> <span class="citation" data-cites="Chung2015">(<a href="#ref-Chung2015" role="doc-biblioref"><strong>Chung2015?</strong></a>)</span>.</p>
</section>
<section id="selected-financial-market-models-and-theory" class="level2">
<h2 class="anchored" data-anchor-id="selected-financial-market-models-and-theory">Selected financial market models and theory</h2>
<p>The following chapter introduces articles that correspond with the subject of the current thesis and are considered as fundamentals of modern finance. Specifically, the beginning contains financial market models. The next subchapter includes basic investment effectiveness indicators that implicitly or explicitly result from the fundamental formulas from the first subchapter.</p>
<section id="capital-asset-pricing-model" class="level3">
<h3 class="anchored" data-anchor-id="capital-asset-pricing-model">Capital Asset Pricing Model</h3>
<p>Works considered as a fundament of quantitative finance and investments are <span class="citation" data-cites="Sharpe1964">Sharpe (<a href="#ref-Sharpe1964" role="doc-biblioref">1964</a>)</span>, <span class="citation" data-cites="Lintner1965">Lintner (<a href="#ref-Lintner1965" role="doc-biblioref">1965</a>)</span>, and <span class="citation" data-cites="Mossin1966">Mossin (<a href="#ref-Mossin1966" role="doc-biblioref">1966</a>)</span>. All these authors, almost simultaneously, formulated Capital Asset Pricing Model (CAPM) that describes dependability between rate of return and its risk, risk of the market portfolio, and risk premium. Assumptions in the model are as follows:</p>
<ul>
<li>Decisions in the model regard only one period,</li>
<li>Market participants has risk aversion, i.e.&nbsp;their utility function is related with plus sign to rate of return, and negatively to variance of portfolio rate of return,</li>
<li>Risk-free rate exists,</li>
<li>Asymmetry of information non-existent,</li>
<li>Lack of speculative transactions,</li>
<li>Lack of transactional costs, taxes included,</li>
<li>Market participants can buy a fraction of the asset,</li>
<li>Both sides are price takers,</li>
<li>Short selling exists,</li>
</ul>
<p>Described by the following model formula is as follows: <span class="math display">\[
E(R_P)=R_F+\frac{\sigma_P}{\sigma_M}\times[E(R_M)-R_F]
\]</span> where:</p>
<ul>
<li><span class="math inline">\(E(R_P)\)</span> – the expected portfolio rate of return,</li>
<li><span class="math inline">\(E(R_M)\)</span> – the expected market rate of return,</li>
<li><span class="math inline">\(R_F\)</span> – risk-free rate,</li>
<li><span class="math inline">\(\sigma_P\)</span> – the standard deviation of the rate of return on the portfolio,</li>
<li><span class="math inline">\(\sigma_M\)</span> – the standard deviation of the rate of return on the market portfolio.</li>
</ul>
<p><span class="math inline">\(E(R_P)\)</span> function is also known as Capital Market Line (CML). Any portfolio lies on that line is effective, i.e.&nbsp;its rate of return corresponds to embedded risk. The next formula includes all portfolios, single assets included. It is also known as Security Market Line (SML) and is given by the following equation: <span class="math display">\[ \label{eq:erl}
E(R_i)=R_F+\beta_i\times[E(R_M)-R_F]
\]</span> where:</p>
<ul>
<li><span class="math inline">\(E(R_i)\)</span> – the expected <span class="math inline">\(i\)</span>-th portfolio rate of return,</li>
<li><span class="math inline">\(E(R_M)\)</span> – the expected market rate of return,</li>
<li><span class="math inline">\(R_F\)</span> – risk-free rate,</li>
<li><span class="math inline">\(\beta_i\)</span> – Beta factor of the <span class="math inline">\(i\)</span>-th portfolio.</li>
</ul>
</section>
</section>
<section id="the-modern-portfolio-theory" class="level2">
<h2 class="anchored" data-anchor-id="the-modern-portfolio-theory">The Modern Portfolio Theory</h2>
<p>The following section discuss the Modern Portfolio Theory developed by Henry Markowitz <span class="citation" data-cites="Markowitz1952">Stulz (<a href="#ref-Markowitz1952" role="doc-biblioref">1995</a>)</span>. The author introduced the model in which the goal (investment criteria) is not only to maximize the return but also to minimize the variance. He claimed that by combining assets in different composition it is possible to obtain the portfolios with the same return but different levels of risk. The risk reduction is possible by diversification, i.e.&nbsp;giving proper weights for each asset in the portfolio. Variance of portfolio value can be effectively reduced by analyzing mutual relations between returns on assets with use of methods in statistics (correlation and covariance matrices). It is important to say that any additional asset in portfolio reduces minimal variance for a given portfolio but it is the correlation what really impacts the magnitude. The Markowitz theory implies that for any assumed expected return there is the only one portfolio that minimizes risk. Alternatively, there is only one portfolio that maximizes return for the assumed risk level. The important term, which is brought in literature, is the effective portfolio, i.e.&nbsp;the one that meets conditions above. The combination of optimal portfolios on the bullet.</p>
<p><img src="../img/markowitz_frontier.jpg" class="img-fluid" alt="Efficient Frontier"> The Markowitz concept is determined by the assumption that investors are risk-averse. This observation is described by the following formula:</p>
<p><span class="math display">\[
E(U)&lt;U(E(X))
\]</span> where:</p>
<ul>
<li><span class="math inline">\(E(U)\)</span> – the expected value of utility from payoff;</li>
<li><span class="math inline">\(U(E(X))\)</span> – utility of the expected value of payoff.</li>
</ul>
<p>The expected value of payoff is given by the following formula: <span class="math display">\[
E(U)=\sum_{i=1}^{n}\pi_iU(c_i)
\]</span> where:</p>
<ul>
<li><span class="math inline">\(\pi_i\)</span> – probability of the <span class="math inline">\(c_i\)</span> payoff,</li>
<li><span class="math inline">\(U(c_i)\)</span> – utility from the <span class="math inline">\(c_i\)</span> payoff.</li>
</ul>
<p>One of the MPT biggest flaws is the fact that it is used for ex post analysis. Correlation between assets changes overtime so results must be recalculated. Real portfolio risk may be underestimated. Also, time window can influence the results.</p>
</section>
<section id="efficient-market-hypothesis" class="level2">
<h2 class="anchored" data-anchor-id="efficient-market-hypothesis">Efficient Market Hypothesis</h2>
<p>In 1965, Eugene Fama introduced the efficient market term. Fama claimed that an efficient market is the one that instanteneously discounts the new information arrival in market price of a given asset. Because this definition applies to financial markets, it determined the further belief that it is not possible to beat the market because assets are correctly priced. Also, if this hypothesis would be true, market participants cannot be better or worse. Their portfolio return would be a function of new, unpredictable information. In that respect, the only role of an investor is to manage his assets so that the risk is acceptable. <span class="citation" data-cites="Fama1965">Fama (<a href="#ref-Fama1965" role="doc-biblioref">1965</a>)</span></p>
<p>It is highly unlikely that EMH exists in its strongest form due to successful quantitative hedge funds that consistetly beat the markets. For instance, Renaissance Capital hedge fund generated on average 40% per annum in the last 30 years <span class="citation" data-cites="Shen2017">Shen (<a href="#ref-Shen2017" role="doc-biblioref">2017</a>)</span>.</p>
<p>Formally, Efficient Market Hypothesis states that a market is efficient with respect to information set <span class="math inline">\(F_t\)</span> if its impossible to make economic profits by trading on the basis of that information set. In other words, it is not possible to achieve any better than risk-adjusted average rate of return. In its essence that claim is consistent with classical price theory <span class="citation" data-cites="Weber2012">Weber (<a href="#ref-Weber2012" role="doc-biblioref">2012</a>)</span>. Over time, other versions (forms) of the EMH has been introduced - weak, semi-strong, and strong <span class="citation" data-cites="Fama1970">Fama, E. F.;Malkiel (<a href="#ref-Fama1970" role="doc-biblioref">1970</a>)</span>.</p>
<p>What means that there is not possibility to make abnormal returns by using the past price movements and volumes to predict the future price movements. However, fundamental analysis might be used to generate such results because the market is not perfect in spotting undervalued and overvalued stocks. Hence, the participants can find profitable companies by researching their financial statements.</p>
<p>It states that neither technical, nor fundamental analysis cannot be exploited for gaining superior returns, and only non-public material information might help in above average results.</p>
<p>The strong form rejects the idea of any possibility to consistently beating the market. According to this idea, any kind information, public or non-public, is completely embedded into current financial asset prices. In other words, there is no advantage for anyone in the market. Returns that deviate from expected values are attributed to pure randomness.</p>
<section id="critic-of-strong-form-of-the-emh" class="level3">
<h3 class="anchored" data-anchor-id="critic-of-strong-form-of-the-emh">Critic of strong form of the EMH</h3>
<p>There are at least a few documented anomalies that contradicts with efficient market hypothesis. For example, price/earnings (P/E) measure can help in systematically outperforming stocks <span class="citation" data-cites="Malkiel2003">Malkiel (<a href="#ref-Malkiel2003" role="doc-biblioref">2003</a>)</span>. The neglected firm effect claims that “uninteresting” companies, often ignored by market analysts are sometimes incorrectly priced, and offer investors potentially fruitful opportunities. Another phenomenon that cannot be explained by the strong form of EMH is so called the January effect <span class="citation" data-cites="Haug2006">Haug and Hirschey (<a href="#ref-Haug2006" role="doc-biblioref">2006</a>)</span>. According to the authors of “The January Effect” working paper, returns reached in January has predictive power for the upcoming 11 months. It persists for both small and large cap companies.</p>
<p>Although the strongest form in its essence is justified, logically correct, it is rather unlikely that it explains the reality, even due to the effects mentioned above.</p>
<!--!Rnw root = ../../masters_thesis.Rnw -->
<!--\SweaveOpts{concordance=TRUE} -->
</section>
</section>
<section id="selected-investment-performance-measures" class="level2">
<h2 class="anchored" data-anchor-id="selected-investment-performance-measures">Selected investment performance measures</h2>
<p>Introduced articles does not include any indicator that would explicitly measure portfolio management effectiveness. Equations that result from the authors’ work are important because some of further developed measures are CAPM-based. The most known are the Sharpe ratio, the Treynor ratio, and the Jensen’s alpha. Popularity of these indicator comes from the fact that they are easy to understand for the average investor. <span class="citation" data-cites="Marte2012">Marte (<a href="#ref-Marte2012" role="doc-biblioref">2012</a>)</span> In <span class="citation" data-cites="Sharpe1966">Sharpe (<a href="#ref-Sharpe1966" role="doc-biblioref">1966</a>)</span>, the author introduced the <span class="math inline">\(\frac{R}{V}\)</span> indicator, also known as the Sharpe Ratio (<span class="math inline">\(S\)</span>), which is given by the following formula: <span class="math display">\[
S_i=\frac{E(R_i-R_F)}{\sigma_i}
\]</span> where:</p>
<ul>
<li><span class="math inline">\(R_i\)</span> – the <span class="math inline">\(i\)</span>-th portfolio rate of return,</li>
<li><span class="math inline">\(R_F\)</span> – risk-free rate</li>
<li><span class="math inline">\(\sigma_i\)</span> – the standard deviation of the rate of return on the <span class="math inline">\(i\)</span>-th portfolio.</li>
</ul>
<p>Treynor (Treynor1965) proposed other approach in which denominator includes <span class="math inline">\(\beta_i\)</span> instead of <span class="math inline">\(\sigma_i\)</span>. The discussed formula is given by: <span class="math display">\[
T_i=\frac{R_i-R_F}{\beta_i}
\]</span> where:</p>
<ul>
<li><span class="math inline">\(R_i\)</span> – the <span class="math inline">\(i\)</span>-th portfolio rate of return,</li>
<li><span class="math inline">\(R_F\)</span> – Risk-free rate</li>
<li><span class="math inline">\(\beta_i\)</span> – Beta factor of the <span class="math inline">\(i\)</span>-th portfolio.</li>
</ul>
<p>Both indicators, i.e.&nbsp;<span class="math inline">\(S\)</span> and <span class="math inline">\(T\)</span> are relative measures. Their value should be compared with a benchmark to determine if a given portfolio is well-managed. If they are higher (lower), it means that analyzed portfolios were better (worse) than a benchmark. The last measure, very popular among market participants, is the Jensen’s alpha. It is given as follows: <span class="math display">\[
\]</span> where:</p>
<ul>
<li><span class="math inline">\(R_i\)</span> – the <span class="math inline">\(i\)</span>-th portfolio rate of return,</li>
<li><span class="math inline">\(R_F\)</span> – Risk-free rate</li>
<li><span class="math inline">\(\beta_i\)</span> – Beta factor of the <span class="math inline">\(i\)</span>-th portfolio.</li>
</ul>
<p>The Jensen’s alpha is an absolute measure and is calculated as the difference between actual and CAPM model-implied rate of return. The greater the value is, the better for the <span class="math inline">\(i\)</span>-th observation.</p>
<p>The differential Sharpe ratio - this measure is a dynamic extension of Sharpe ratio. By using the indicator, it can be possible to capture a marginal impact of return at time t on the Sharpe Ratio. The procedure of computing it starts with the following two formulas: <span class="math display">\[
A_n=\frac{1}{n}R_n+\frac{n-1}{n}A_{n-1}
\]</span> <span class="math display">\[
B_n=\frac{1}{n}R_n^2+\frac{n-1}{n}B_{n-1}
\]</span> At <span class="math inline">\(t=0\)</span> both values equal to 0. They serve as the base for calculating the actual measure - an exponentially moving Sharpe ratio on <span class="math inline">\(\eta\)</span> time scale. <span class="math display">\[
S_t=\frac{A_t}{K_\eta\sqrt{B_t-A_t^2}}
\]</span> where:</p>
<ul>
<li><span class="math inline">\(A_t=\eta R_t+(1-\eta)A_{t_1}\)</span></li>
<li><span class="math inline">\(B_t=\eta R_t^2+(1-\eta)B_{t_1}\)</span></li>
<li><span class="math inline">\(K_\eta=(\frac{1-\frac{\eta}{2}}{1-\eta})\)</span></li>
</ul>
<p>Using of the differential Sharpe ratio in algorithmic systems is highly desirable due to the following features <span class="citation" data-cites="MoodyWu1997">Moody, John E.; Wu (<a href="#ref-MoodyWu1997" role="doc-biblioref">1997</a>)</span>:</p>
<ul>
<li>Recursive updating - it is not needed to recompute the mean and standard deviation of returns every time the measure value is evaluated. Formula for <span class="math inline">\(A_t\)</span> (<span class="math inline">\(B_t\)</span>) enables to very straightforward calculation of the exponential moving Sharpe ratio, just by updating for <span class="math inline">\(R_t\)</span> (<span class="math inline">\(R_t^2\)</span>)</li>
<li>Efficient on-line optimization - the way the formula is provided directs to very fast computation of the whole statistic with just updating the most recent values</li>
<li>Interpretability - the differential Sharpe ratio can be easily explained, i.e. it measures how the most recent return affect the Sharpe ratio (risk and reward).</li>
</ul>
<p>The drawdown is the measure of the decline from a historical peak in an asset. The formula is given as follows:</p>
<p><span class="math display">\[
D(T)=\max\{max_{0, t\in (0,T)} X(t)-X(\tau)\}
\]</span></p>
<p>The Sterling ratio (SR)</p>
<p>The maximum drawdown (MDD) at time <span class="math inline">\(T\)</span> is the maximum of the Drawdown over the asset history. The formula is given as follows:</p>
<p><span class="math display">\[
MDD(T)=\max_{\tau\in (0,T)}[\max_{t\in (0,\tau)} X(t)-X(\tau)]
\]</span></p>
<p>In this chapter term <strong>machine learning</strong> and its subfields are explained. Discussion also contains possible applications for trading financial instruments.</p>
</section>
</section>
<section id="machine-learning" class="level1">
<h1>Machine Learning</h1>
<p>As the field evolves, there are many definitions of machine learning sources provide. In this subchapter, the author has arbitrarly selected definitions that accurately captures the spirit of the discipline. What is machine learning then? The most accepted and widely used definitions are as follows:</p>
<ul>
<li>“Field of study that gives computers the ability to learn without being explicitly programmed.” - Arthur Samuel, a pioneer in machine learning and computer gaming <span class="citation" data-cites="Samuel1959">Samuel (<a href="#ref-Samuel1959" role="doc-biblioref">1959</a>)</span></li>
<li>“A computer program is said to learn from experience <span class="math inline">\(E\)</span> with respect to some class of tasks <span class="math inline">\(T\)</span> and performance measure <span class="math inline">\(P\)</span>, if its performance at tasks in <span class="math inline">\(T\)</span>, as measured by <span class="math inline">\(P\)</span>, improves with experience <span class="math inline">\(E\)</span>.” - Tom Mitchell, a computer scientist and E. Fredkin University Professor at the Carnegie Mellon University (CMU) <span class="citation" data-cites="Mitchell1997">Mitchell (<a href="#ref-Mitchell1997" role="doc-biblioref">1997</a>)</span></li>
</ul>
<p>Especially the latter is considered as an elegant and modern definition. Less formal, but also relevant remarks, comes from two authors of textbooks from the discipline:</p>
<ul>
<li>“Pattern recognition has its origins in engineering, whereas machine learning grew out of computer science. However, these activities can be viewed as two facets of the same field…” - Christopher Bishop</li>
<li>“One of the most interesting features of machine learning is that it lies on the boundary of several different academic disciplines, principally computer science, statistics, mathematics, and engineering. …machine learning is usually studied as part of artificial intelligence, which puts it firmly into computer science …understanding why these algorithms work requires a certain amount of statistical and mathematical sophistication that is often missing from computer science undergraduates.” - Stephen Marsland <span class="citation" data-cites="Marsland2009">Marsland (<a href="#ref-Marsland2009" role="doc-biblioref">2009</a>)</span></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../img/data_science.png" height="400" class="figure-img"></p>
<figcaption>Data Science Graph</figcaption>
</figure>
</div>
<p>Despite many more concepts, ideas, and comments as to what exactly machine learning is, the general goal is the same: Machine learning is about building such models that resemble the reality to a sufficient extent, are optimal in terms of a value function and can be later used for predictions on new data.</p>
<section id="why-is-machine-learning-important" class="level3">
<h3 class="anchored" data-anchor-id="why-is-machine-learning-important">Why is machine learning important?</h3>
<p>Machine learning helps in solving problems that are difficult or even impossible to solve in a determinisic way. <span class="citation" data-cites="Jason2013">Jason (<a href="#ref-Jason2013" role="doc-biblioref">2013</a>)</span> Sometimes variables can be missing or observed values can contain an embedded error. Traditional models are often prone to be under- or overdetemined. They might not generalize well or are too general. An appropriate machine learning model should contain approximate solution containing only relevant parts.</p>
</section>
<section id="classification-of-machine-learning-algorithms" class="level3">
<h3 class="anchored" data-anchor-id="classification-of-machine-learning-algorithms">Classification of machine learning algorithms</h3>
<p>In machine learning (ML), tasks are classified into broader categories based on how learning/feedback (<span class="math inline">\(P\)</span>) is received and/or what kind of problem they solve. One can distinct the following ones:</p>
<ul>
<li><p>Supervised Learning - the whole set <span class="math inline">\((Y_t;X_{t, 1}, ..., X_{t,n})\)</span> is available. The goal is to model the special variable <span class="math inline">\(Y_t\)</span> using a subset of <span class="math inline">\(X_t\)</span> variables, i.e.&nbsp;find a functional relationship <span class="math inline">\(Y_t = f(\mathbb{X_t})\)</span> between the input variables and the output variables which minimizes a predefined loss function <span class="math inline">\(g(f(\mathbb{X}_t);Yt)\)</span>. The structural form of this relationship is constrained by the class of functions considered. For example we can assume that there is a linear relationship between input and output variables and a square loss function, then the problem becomes: <span class="math display">\[\min_{b1\dots bn}\mathbb{E[}(Y_t-(b_1X_{t,1}+\dots+b_nX_{t,n}))^2]\]</span></p>
<p>The utilized estimation method above is called least squares method for linear regression. Even though it is considered a simple one, it sometimes provides sufficient results. Other popular methods for supervised learning are:</p>
<ul>
<li>K-nearest neighboors, Neural Networks,</li>
<li>SVM - Support Vector Machines,</li>
<li>Random Forests</li>
</ul></li>
<li><p>Unsupervised learning - it is the category that deals with only <span class="math inline">\(\mathbb{X_t}\)</span> set. In other words, The goal is to find patterns among the dataset and categorize observations. The most popular methods are:</p>
<ul>
<li>Clustering - based on finding groups of instances which are similar as possible to observations from the same groups while as different as possible to observations from other ones</li>
<li>Feature extraction - this subcategory of unsupervised learning consists of methods for extracting relevant variables from a set of variables <span class="math inline">\(\mathbb{X}_t\)</span>. Often, a subset of a dataset can contain a similar amount of information as the original one while reducing dimensionality so that a model computation is much faster and efficient. improves the model in Occam’s Razor sense.</li>
<li>Anomaly detection - this type helps in identification of observations that are outliers and should be carefully investigated. Sometimes the whole variable needs to be transformed or spotted observations must be removed due to their invalidity.</li>
</ul></li>
<li><p>Reinforcement Learning - it is probably the most intuitive category of ML in terms of what people implicitly believe to be artificial intelligence. According to <span class="citation" data-cites="Silver2017">(<a href="#ref-Silver2017" role="doc-biblioref"><strong>Silver2017?</strong></a>)</span>, it captures influences from disciplines such as engineering, economics, mathematics, neuroscience, psychology and computer science. Algorithms in reinforcement learning maximize long-term cumulated reward and <strong>interacts with the environment</strong>, i.e.&nbsp;are convenient when a problem is not stationary.</p>
<p>The two most specific features of reinforcement learning algorithms are trial-and-error and delayed rewards what means that this type of ML uses training information to evaluate the actions rather than instructs by giving definitive actions. This is what distinguishes reinforcement learning from supervised learning and is one of the reasons why it is considered as a subfield in ML. Moreover, it does not base on a training set of labeled examples. In SL, each observation is strictly specified as to what an algorithm should do. For instance if blue balls according to the model should be in blue basket, they will always end up there. Supervised learning goal is to generalize well on the training data so that the formula works also for the test data. It is important and the most researched area of ML nowadays, however it is not enough when interaction between an agent and an environment take place. In such problems an agent should learn from its own actions, sense states, and gain experience.</p>
<p>Reinforcement learning need to be distincted from unsupervised learning as well. UL is focused on finding structures not explicitly given by collections of unlabeled datasets. It sounds similar, but it is far from RL, where the whole idea is to maximize sum of reward signals. Finding data patterns might be useful (as stated in the bullet point about unsupervised learning), but it does not solve a RL problem. Hence, the approach analyzed in the thesis should be considered as a next paradigm, seperated paradigm. The only feedback an agent receives is a scalar reward. The goal of it is to maximize long-run value function which consists of summed up (discounted) rewards in subsequent states. The goal of the agent is to learn by trial-and-error which actions maximize his long-run rewards. The environment changes stochastically and in some cases interacts with the agent. The agent must choose such a policy that optimizes amount of rewards it receives. The design must capture this fact by adjusting the agent so that it does not act greedily, i.e.&nbsp;it should explore new actions instead of exploiting existing optimal (possibly suboptimal) solutions.</p></li>
</ul>
</section>
<section id="selected-aspects-of-reinforcement-learning" class="level2">
<h2 class="anchored" data-anchor-id="selected-aspects-of-reinforcement-learning">Selected aspects of reinforcement learning</h2>
<p>In the following section the author discussed relevant aspects and challengees of the paradigm.</p>
<section id="explorationexploitation" class="level3">
<h3 class="anchored" data-anchor-id="explorationexploitation">Exploration/exploitation</h3>
<p>One of the most important problems in RL is the trade-off between exploration and exploitation. To maximize cumulated rewards an agent should take actions that worked in the past and caused bigger payoffs (exploit). At the very beginning of learning process it never knows what works well, though. Hence, it needs to discover desirable actions for its state (explore). The dilemma is unresolved as of now, there are at least a few approaches to tackle the problem, though. In the next subsection the author presents that possible methods on the example of Bandit problem.</p>
<section id="epsilon-greedy-policy" class="level4">
<h4 class="anchored" data-anchor-id="epsilon-greedy-policy"><span class="math inline">\(\epsilon\)</span>-greedy policy</h4>
<p>The simplest version is to behave greedily most of the time, i.e.&nbsp;an agent selects such action (<span class="math inline">\(A_t\)</span>) that maximizes the used value function (e.g.&nbsp;<span class="math inline">\(Q_t(a)\)</span>, but sometimes, with probability of <span class="math inline">\(\epsilon\)</span> pick up a random action from those available, apart from the action value estimates. Such an algorithm guarantees that every action for every state will be explored and eventually <span class="math inline">\(Q_t(a)=q_*(a)\)</span>. It implies that probability of choosing the most optimal action will converge to more than <span class="math inline">\(1-\epsilon\)</span>, to near certainty. The disadvantage of this simple method is that it says very little of its practical effectiveness. Asymptotic guarantee might take too long in a real environment. It can be shown that small <span class="math inline">\(\epsilon\)</span> causes the agent to gain more reward at initial steps, but tends to underperform against larger <span class="math inline">\(\epsilon\)</span> values when number of steps is getting larger.</p>
</section>
<section id="optimistic-initial-values" class="level4">
<h4 class="anchored" data-anchor-id="optimistic-initial-values">Optimistic initial values</h4>
<p>One of the techniques to improve agent’s choices is based on the idea of encouraging the agent to explore. Why is that? If the actual reward is smaller than initially set up action-value methods, an agent is more likely to pick up actions that potentially can stop getting rewards that constantly worsen value function <span class="math inline">\(q(a)\)</span>. Eventually, the system does a lot more exploration even if greedy actions are selected all the time.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../img/optimistic_initial_values.png" height="400" class="figure-img"></p>
<figcaption>The effect of optimistic initial action-value estimates on the 10-armed testbed</figcaption>
</figure>
</div>
</section>
<section id="upper-confidence-bound-action-selection" class="level4">
<h4 class="anchored" data-anchor-id="upper-confidence-bound-action-selection">Upper-Confidence-Bound Action Selection</h4>
<p>The other method for handling the exploration/exploitation problem is by using the special bounds that narrow with the number of steps taken. The formula is as follows:</p>
<p><span class="math display">\[A_t = arg\max_a[Q_t(a)+c\sqrt\frac{ln_t}{N_t(a)}\]</span> where:</p>
<ul>
<li><span class="math inline">\(ln_t\)</span> is the natural logarithm of $t%</li>
<li>$N_t(a) - the number of times that action a has been selected prior to time <span class="math inline">\(t\)</span></li>
<li><span class="math inline">\(c\)</span> - the exploration rate</li>
</ul>
<p>The idea of this soltuion is that the square-root part is an uncertainty measure or variance in the <span class="math inline">\(a\)</span> estimation. The use of the natural logarithm implies that overtime square-root term, so does the confidence interval, is getting smaller. All actions will be selected at some point, but the ones with non-optimal values for <span class="math inline">\(Q(a)\)</span> are going to be selected much less frequently over time. UCB performs well, but it is harder to apply (generalize) for a broader amount of problems than <span class="math inline">\(\epsilon\)</span>-greedy algorithm. Especially, when one is dealing with nonstationary problems. In such situations, algorithms more complex than those presented in this subsection should be selected.</p>
<p><img src="../img/ucb_vs_egreedy.png" height="400" alt="Average performance of UCB action selection on the 10-armed testbed"> <!-- . As shown, UCB generally --> <!-- performs better than "-greedy action selection, except in the rst k steps, when it selects randomly among the --> <!-- as-yet-untried actions. --> <!-- In this part  --></p>
<p>Reinforcement learning algorithms can be classified into three general subcategories:</p>
<ul>
<li>Model Based - they are based on the idea that an model of the environment is known. Actions are chosen by searching and planning in this model. Markov Decision Process (MDP) is a typical example of such method since it requires knowledge of the Markov transition matrix and reward function.</li>
<li>Model-free - it uses experience to learn in a direct way from state-action values or policies. They can achieve the same behaviour, but without any knowledge on the world model an agent acts in. In practical examples, reinforcement learning is primarily used for environments where a transition matrix is not known. Given a policy, a state has some value, which is defined as cumulated utility (reward) starting from the state. Model-free methods are generally less efficient than model-based ones because information about the environment is combined with possibly incorrect estimates about state values <span class="citation" data-cites="Dayan2008">Dayan and Niv (<a href="#ref-Dayan2008" role="doc-biblioref">2008</a>)</span>.</li>
</ul>
</section>
</section>
<section id="model-based-methods-in-reinforcement-learning" class="level3">
<h3 class="anchored" data-anchor-id="model-based-methods-in-reinforcement-learning">Model-based Methods in Reinforcement Learning</h3>
<ul>
<li>Value iteration - a model-based algorithm that computes the optimal state value function by improving the estimate of <span class="math inline">\(V(s)\)</span>. It starts with initializing arbirary values and then updates <span class="math inline">\(Q(s, a)\)</span> and <span class="math inline">\(V(s)\)</span> values until they converge. The pseudocode is as follows:</li>
</ul>
<p><img src="../img/value_iteration.png" class="img-fluid" alt="Value Iteration Algorithm. Based on Sutton and Barto (2017)"> <!-- \begin{algorithm}[H] --> <!--  \KwResult{how to write algorithm with} --> <!--  Initialize $V(s)$ to arbitrary values\; --> <!--   Repeat\; --> <!--     \For{$all\space s \in S$}{ --> <!--       \For{$all\space a \in A$}{ --> <!--         $Q(s, a) \leftarrow E[r|s, a] + \gamma\sum_{s e S}P(s'|s, a)V(s^{'})$\; --> <!--       $V(s) \leftarrow max_a Q(s,a)$\; --> <!--       } --> <!--   } --> <!--   Until $V(s)$ converge\; --> <!--   \caption{Value Iteration Algorithm. Based on @Alpaydin2013} --> <!-- \end{algorithm} --></p>
<ul>
<li>Policy iteration - while in the previous bullet the algorithm is improving value function, policy iteration is based on the different approach. Concretely, there are two functions to be optimized <span class="math inline">\(V^{\pi}(s)\)</span> and <span class="math inline">\(\pi^{'}(s)\)</span>. This method is based on the premise that a RL agent cares about finding out the right policy. Sometimes, it is more convenient to directly use policies as a function of states. Below is the pseudocode:</li>
</ul>
<!-- \begin{algorithm}[H] -->
<!--  \KwResult{how to write algorithm with} -->
<!--  Initialize $\pi^{'}$ to arbitrary values\; -->
<!--   Repeat\; -->
<!--   \indent $\pi \leftarrow \pi^{'}$ -->
<!--   \indent Compute the values using $\pi$ by solving the linear equations\; -->
<!--   \indent $V^{\pi}(s) = E[r|s, \pi(s)] + \gamma\sum{s^{'} e S}P(s^{'}|s, \pi(s))V^{\pi}(s^{'})$ -->
<!--   \indent Improve the policy at each state -->
<!--   \indent $\pi^{'}(s) <- argmax_{a}(E[r|s, a] + \gamma\sum_{s^{'} e S}P(s^{'}|s, a)V^{\pi}(s^{'}))$ -->
<!--   Until $\pi=\pi^{'}$\; -->
<!--   \caption{Policy Iteration Algorithm. Based on @Alpaydin2013} -->
<!-- \end{algorithm} -->
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../img/policy_iteration.png" class="img-fluid figure-img"></p>
<figcaption>Policy Iteration Algorithm. Based on <span class="citation" data-cites="Sutton2017">Sutton and Barto (<a href="#ref-Sutton2017" role="doc-biblioref">2017</a>)</span></figcaption>
</figure>
</div>
</section>
<section id="model-free-learning" class="level3">
<h3 class="anchored" data-anchor-id="model-free-learning">Model Free Learning</h3>
<p>Model Free learning is a subcategory of reinforcement learning algorithms which are used when a model is not known. The agent improves its accuracy in choosing right actions by interacting with the environment without explicit knowledge of the underlying transition matrix. It fits trading conditions - in financial markets it is impossible to know what the model is and what probabilities in the transition matrix are (they are not stationary). Hence, value or policy iteration algorithm can not be used directly.</p>
<p>Even though, Markov Decision Process and its element are not visible, the agent can gain experience from sampled states. It is assumed that eventually the distribution of sampled states will converge to the one in the transition matrix. So do <span class="math inline">\(Q(s, a)\)</span> converge to <span class="math inline">\(Q^{*}\)</span> and <span class="math inline">\(\pi^{*}\)</span> to the optimal policy. The conditions required by the convergence is that all state-action pairs were visited infinite times and the agent is greedy once it finds the best action in every state.</p>
</section>
<section id="components-of-an-reinforcement-learning-system" class="level3">
<h3 class="anchored" data-anchor-id="components-of-an-reinforcement-learning-system">Components of an reinforcement learning system</h3>
<p>Reinforcement learning systems are developed to solve sequential decision making problems, to select such actions that eventually maximize cumulative discounted future rewards. In the following section the author explained components of reinforcement learning on the example of game of chess and trading. The subsection was partially inspired and based on <span class="citation" data-cites="Sutton2017">Sutton and Barto (<a href="#ref-Sutton2017" role="doc-biblioref">2017</a>)</span>.</p>
<ul>
<li><p>Environment (<span class="math inline">\(E\)</span>) - it defines what states and actions are possible. In the game of chess it is the whole set of rules and possible combination of figures on the chessboard. It must be stated that some states are not available and will be never reached. In trading such rules might constitute that for instance the only position an agent can take is 0 or 1, or that weights of assets in a portfolio must sum up to 1.</p></li>
<li><p>State (<span class="math inline">\(s\)</span>) - can be seen as a snapshot of the environment. It contains a set of information in time <span class="math inline">\(t\)</span> that a RL agent uses to pick the next action. States can be terminal, i.e.&nbsp;the agent will no longer be able to choose any action. In such scenario they end an episode (epoch), a sequence of state-action pairs from the start to the end of the game. For a trading application, a state in time <span class="math inline">\(t\)</span> can be a vector of different financial measures, such as rate of return, implied/realized volatility, moving averages, economics measures, technical indicators, market sentiment measures, etc.</p></li>
<li><p>Action (<span class="math inline">\(a\)</span>) - givn a current state the agent chooses an action which directs him into a new state, either deterministically or stochastically. The action choice process itself may also be deterministic or based on probability distributions. In the game of chess analogy, an action is to move a figure in accordance to the game’s rules. In trading it could be for instance going long, short, staying flat, outweighing.</p></li>
<li><p>Policy (<span class="math inline">\(\pi\)</span>) - a policy is a mapping from state of the environment to action to be taken in that state. In psychology it is called a set of stimulus, i.e.&nbsp;response rules or associations. The policy might be a lookup table or a simple function (e.g.&nbsp;linear), but not necessarily. Especially in trading where variables are often continous extensive computations to set up a satisfying outcome take place. The policy is the most essential part of a reinforcement learning agent because they determine how it behaves. It may be stochastic. Policies do not imply deterministic nature of the mapping. Even after countless number of episodes and states, there is a chance that an efficient RL algorithm will explore other states rather than by exploiting the then-optimal action</p></li>
<li><p>Value Function - it is a prediction of future, usually discounted rewards. Value functions are used for determining how much a state should be desired by the agent. They depend on initial states (<span class="math inline">\(S_0\)</span>), and a policy that is picked up by the agent. Every state should have an associated value, even if the path it is part of was never explored - in such cases they usually equal to zero. The general formula for value function is as follows:</p></li>
</ul>
<p><span class="math display">\[V^\pi=\mathbb{E}_\pi[\sum\limits_{k=1}^\infty \gamma^kr_{t+k}|s_t=s]\]</span></p>
<p>where <span class="math inline">\(\gamma\)</span> is a discount factor from the range <span class="math inline">\([0; 1]\)</span>. It measures how much more instant rewards are valued. The smaller it is the more immediate values are relatively more relevant and cause algorithm to be more greedy. Sometimes <span class="math inline">\(\gamma\)</span> is equal to 1 if it is justified by the design of the whole agent.</p>
<p>Value estimation, as a area of research in RL is probably the most vital one in the last decade. The most important distinction between different RL algorithms lies as to how it is calculated, in what form, and what variables it incorporates.</p>
<ul>
<li><p>Reward (<span class="math inline">\(r\)</span>) - rewards are the essence of reinforcement learning predictions. Value function, as previously stated, is a sum of, often discounted, rewards. Without them, as components of value function, an agent would not be able to spot (or optimal) better policies actions are based on. Hence, it is assumed rewards are the central point, required element, of every RL algorithm. Rewards are always given as a scalar, single value that is retrieved from the environment, that is easy to observe and interpret. With value function it is much harder since it can be obtained only by calculating a sequence of observations a RL agent makes over its lifetime.</p></li>
<li><p>Model (<span class="math inline">\(m\)</span>) - a model shows the dynamics of environment, how it will evolve from <span class="math inline">\(S_{t-1}\)</span> to <span class="math inline">\(S_t\)</span>. The model helps in predicting what the next state and next reward will be. They are used for planning, i.e.&nbsp;trial-and-error approach is not needed in order to achieving the optimal policy. Formally, it is a set of transition matrices:</p></li>
</ul>
<p><span class="math display">\[\mathbb{P}_{ss^{'}}^a=\mathbb{P}[s^{'}|s,a]\]</span> <span class="math display">\[\mathbb{R}_s^a=\mathbb{E}[r|s,a]\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\mathbb{P}_ss^{'}{a}\)</span> is a matrix of probability of transitions from state <span class="math inline">\(s\)</span> to state <span class="math inline">\(s^{'}\)</span> when taking action <span class="math inline">\(a\)</span>. Analogously, <span class="math inline">\(\mathbb{R}_{s}^a\)</span> is an expected value of reward when an agent is in state <span class="math inline">\(s\)</span> and taking action <span class="math inline">\(a\)</span></li>
</ul>
</section>
<section id="limitations" class="level3">
<h3 class="anchored" data-anchor-id="limitations">Limitations</h3>
<p>Reinforcement learning is not a panacea for all kinds of ML problems, they should be heavily associated with problems based on states, some policy to determine and defined value function. A state is just a signal that reaches the agent, a snapshot of the environment at a time. Most of pure reinforcement learning methods are oriented about states and their values. Even though it is useful for simpler environments, for more sophisticated ones it is not as easy. First of all, tabular data is not a good way to store information about expected values for states/states-actions. They would not fit into memory as variables are continuous. Hence, some additional approaches must be used in order to solving the problem efficiently.</p>
</section>
</section>
</section>
<section id="research" class="level1">
<h1>Research</h1>
<p>In the following chapter the author designed a research based on reinforcement learning agents for FX market. The chapter starts with outlining research objectives, then used dataset is explained. In subsequent parts, it is explained how the research was conducted, what elements and methods have been chosen from the previous chapters. The last part consists of results, what exactly have been achieved and if it conforms with expectations.</p>
<section id="research-objective" class="level2">
<h2 class="anchored" data-anchor-id="research-objective">Research Objective</h2>
<p>The primary research goal was to evaluate the Reinforcement Learning-based algorithm for multiasset trading. The main idea behind the algorithm deployment is that it can systematically outperform benchmarks in terms of selected risk and return measures. Designed trading system was aimed to spot non-trivial patterns in data, more efficiently than human, and exploit them accordingly.</p>
<p>In the project author wanted to assess the possibility of using a reinforcement learning agent for trading domain. The objectives are as follows</p>
<ul>
<li>Implementation - consisting of creating reinforcement learning-based agent that are capable of trading financial instruments basing on time series tables</li>
<li>Evaluation - testing if trading agents for out-of-sample periods can outperform benchmarks in the measures provided by the author</li>
<li>Conclusion - answering the question if such approach might help in generating abnormal positive results and determining if the method can be feasible and efficient in real-like environment</li>
</ul>
</section>
<section id="design-of-the-research" class="level2">
<h2 class="anchored" data-anchor-id="design-of-the-research">Design of the research</h2>
<p>The whole system can be divided into three main parts:</p>
<ul>
<li>Data preprocessing - taking FX data from Bloomberg with use of the dedicated API, parsing the data and adjusting it for the further analysis. The system is dedicated for currency trading, however with little adjustments it could fit in other asset classes as well.</li>
<li>Variable extraction - not all preprocessed currency pairs are relevant and worth adding. For instance, if <span class="math inline">\(USD/CNH\)</span> is highly correlated with <span class="math inline">\(USD/CNY\)</span> it is senseless to add the latter to the portfolio. #TO DO</li>
<li>State-action space - the extracted variables, based on time series for currency pairs, are merged into state space</li>
</ul>
<section id="assumption" class="level3">
<h3 class="anchored" data-anchor-id="assumption">Assumption</h3>
<p>In the work, the author has assumed that:</p>
<ul>
<li>Zero slippage - the FX market is liquidity is good enough that there the execution price is equal to the price shown by the venue (Bloomberg)</li>
<li>Zero market impact - trades executed by the agent are not big enough that they can move the market and cause significant market impact</li>
</ul>
</section>
</section>
<section id="data-preparation" class="level2">
<h2 class="anchored" data-anchor-id="data-preparation">Data Preparation</h2>
<p>The original data set consisted of tick data of <strong>52</strong> currency pairs in three months ( observations) between November, 2017 and March, 2018. There are 3 variables for each of them:</p>
<ul>
<li>Timestamp - usually given as a UNIX timestamp (starting from 1st of January of 1970) with precision of milliseconds or microseconds</li>
<li>Bid price - the highest price that a buyer is willing to pay for a given amount of a currency</li>
<li>Ask price - the lowest price that a seller is willing to accept for a given amount of a currency</li>
<li>Mid price - the price calculated as <span class="math inline">\(MID_PRICE = frac{BID_PRICE + ASK_PRICE}{2}\)</span>. Usually rounded up to four or five decimals (depending on currency’s liquidity, value against a non-base currency)</li>
</ul>
<p>The ticks were left untrasnformed. The purpose was to test the method in a real-life environment. Hence, e.g.&nbsp;aggregating would ruin the initial idea.</p>
<p>Bid and ask prices are taken from Bloomberg’s FXGo platform using R Bloomberg API (RBlpapi) for 1 mio of base currency. Bloomberg covers prices from hundreds of banks and for most currency pairs in the world. Some of them are crossed, with use of Euro or US Dollar, for instance <strong>EUR/JPY</strong> is the combination of:</p>
<p><span class="math display">\[EUR/JPY = EUR/USD\times USD/JPY\]</span></p>
<p>Sometimes pairs can effectively consist of 3 parts (legs), for instance <strong>PLN/MXN</strong>: <span class="math display">\[PLN/MXN = (USD/MXN\times EUR/USD)/(EUR/PLN)\]</span></p>
<p>Using such pairs is usually problematic and reduces reliability in backtesting. Hence, for the purpose of the work only the most liquid pairs, based on G10 currencies, had been used:</p>
<ul>
<li>USD - United States dollar</li>
<li>EUR - Euro</li>
<li>JPY - Japanese yen</li>
<li>GBP - Pound sterling</li>
<li>CHF - Swiss franc</li>
<li>AUD - Australian dollar</li>
<li>NZD - New Zealand dollar</li>
<li>CAD - Canadian dollar</li>
<li>SEK - Swedish krona</li>
<li>NOK - Norwegian krona</li>
</ul>
<p>In eFX the crucial element is spread, calculated as the difference between bid and ask prices. It depends on several factors, such as time of the day, one-off events, volatility, ability of liquidity providers to warehouse risk, or market sentiment. The data source had been selected so that it captured spread and reflected FX market as good as possible.</p>
<p>Below is the glimpse of the data used in the research:</p>
<p>Cols: average spread (in USD), min spread, max spread, average number of ticks, mid price movements</p>
<p>Rows: time of the day, sum</p>
<p>In FX market participants usually get quotes for different levels (tiers). The assumption of the work was to use the smallest one to reduce possible market impact.</p>
</section>
<section id="code-and-the-research-process" class="level2">
<h2 class="anchored" data-anchor-id="code-and-the-research-process">Code and the Research Process</h2>
<p>The implementation of trading agents was based on R (both base and external libraries).</p>
<p>The graphical presentation was prepared with the use of ggplot2 library.</p>
<p>The R-project consists of:</p>
<ul>
<li>frun.R,</li>
<li>get_data.R,</li>
<li>cointegration.R,</li>
<li>attributes.R,</li>
<li>discretization.R,</li>
<li>functions.R,</li>
<li>state_space.R,</li>
<li>mcc.R,</li>
<li>q_learning.R</li>
</ul>
<p>The main part was based on run.R script. Running it merges all above-mentioned scripts and executes the whole experiment.</p>
<div style="page-break-after: always;"></div>
</section>
</section>
<section id="bibliography" class="level1 unnumbered">


</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">Bibliography</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-Dayan2008" class="csl-entry" role="listitem">
Dayan, Peter, and Yael Niv. 2008. <span>“<span class="nocase">Reinforcement learning: The Good, The Bad and The Ugly</span>.”</span> <a href="https://doi.org/10.1016/j.conb.2008.08.003">https://doi.org/10.1016/j.conb.2008.08.003</a>.
</div>
<div id="ref-Fama1970" class="csl-entry" role="listitem">
Fama, E. F.;Malkiel, B. G. 1970. <span>“<span class="nocase">Efficient capital markets: A review of theory and empirical work</span>.”</span> <em>Journal of Finance</em> 25 (2): 383–417.
</div>
<div id="ref-Fama1965" class="csl-entry" role="listitem">
Fama, Eugene F. 1965. <span>“<span class="nocase">The Behavior of Stock-Market Prices</span>.”</span> <em>The Journal of Business</em> 38 (1): 34. <a href="https://doi.org/10.1086/294743">https://doi.org/10.1086/294743</a>.
</div>
<div id="ref-Haug2006" class="csl-entry" role="listitem">
Haug, Mark, and Mark Hirschey. 2006. <span>“<span class="nocase">The january effect</span>.”</span> <a href="https://doi.org/10.2469/faj.v62.n5.4284">https://doi.org/10.2469/faj.v62.n5.4284</a>.
</div>
<div id="ref-Jason2013" class="csl-entry" role="listitem">
Jason, Brownlee. 2013. <span>“<span class="nocase">What is Machine Learning?</span>”</span> <a href="https://machinelearningmastery.com/what-is-machine-learning/">https://machinelearningmastery.com/what-is-machine-learning/</a>.
</div>
<div id="ref-Lintner1965" class="csl-entry" role="listitem">
Lintner, John. 1965. <span>“<span class="nocase">Security prices, risk, and maximal gains from diversifivation</span>.”</span> <em>Jf</em> 20 (4): 587–615. <a href="https://doi.org/10.1111/j.1540-6261.1965.tb02930.x">https://doi.org/10.1111/j.1540-6261.1965.tb02930.x</a>.
</div>
<div id="ref-Lyons2002" class="csl-entry" role="listitem">
Lyons, Richard K. 2002. <span>“<span class="nocase">The Microstructure Approach to Exchange Rates (a review)</span>.”</span> <em>Financial Analysts Journal</em> 58 (5): 101–3. <a href="https://doi.org/10.2469/faj.v58.n5.2475">https://doi.org/10.2469/faj.v58.n5.2475</a>.
</div>
<div id="ref-Malkiel2003" class="csl-entry" role="listitem">
Malkiel, Burton G. 2003. <span>“<span class="nocase">The Efficient Market Hypothesis and Its Critics</span>.”</span> <em>Journal of Economic Perspectives</em> 17 (1): 59–82. <a href="https://doi.org/10.1257/089533003321164958">https://doi.org/10.1257/089533003321164958</a>.
</div>
<div id="ref-Marsland2009" class="csl-entry" role="listitem">
Marsland, Stephen. 2009. <em><span>Machine Learning: An Algorithmic Perspective</span></em>. <a href="https://doi.org/10.1111/j.1751-5823.2010.00118_11.x">https://doi.org/10.1111/j.1751-5823.2010.00118_11.x</a>.
</div>
<div id="ref-Marte2012" class="csl-entry" role="listitem">
Marte, Jonnelle. 2012. <span>“<span>No Title</span>.”</span> <em>Wall Street Journal</em>.
</div>
<div id="ref-Mitchell1997" class="csl-entry" role="listitem">
Mitchell, Tom M. 1997. <em><span>Machine Learning</span></em>. 1. <a href="https://doi.org/10.1145/242224.242229">https://doi.org/10.1145/242224.242229</a>.
</div>
<div id="ref-MoodyWu1997" class="csl-entry" role="listitem">
Moody, John E.; Wu, L. 1997. <span>“<span class="nocase">Optimization of trading systems and portfolios</span>.”</span> London.
</div>
<div id="ref-Mosic2017" class="csl-entry" role="listitem">
Mosic, Ranko. 2017. <span>“<span class="nocase">Deep Reinforcement Learning Based Trading Application at JP Morgan Chase</span>.”</span> <em>Medium</em>. <a href="https://medium.com/@ranko.mosic/reinforcement-learning-based-trading-application-at-jp-morgan-chase-f829b8ec54f2">https://medium.com/@ranko.mosic/reinforcement-learning-based-trading-application-at-jp-morgan-chase-f829b8ec54f2</a>.
</div>
<div id="ref-Mossin1966" class="csl-entry" role="listitem">
Mossin, Jan. 1966. <span>“<span class="nocase">Equilibrium in a capital asset market</span>.”</span> <em>Econometrica</em> 34 (4): 768–83. <a href="https://doi.org/10.2307/1910098">https://doi.org/10.2307/1910098</a>.
</div>
<div id="ref-Samuel1959" class="csl-entry" role="listitem">
Samuel, Arthur L. 1959. <span>“<span class="nocase">Some Studies in Machine Learning Using the Game of Checkers Some Studies in Machine Learning Using the Game of Checkers</span>.”</span> <em>IBM Journal</em> 3. <a href="https://www.cs.virginia.edu/{~}evans/greatworks/samuel1959.pdf">https://www.cs.virginia.edu/{~}evans/greatworks/samuel1959.pdf</a>.
</div>
<div id="ref-Sharpe1964" class="csl-entry" role="listitem">
Sharpe, William F. 1964. <span>“<span class="nocase">Capital Asset Prices: A Theory of Market Equilibrium under Conditions of Risk</span>.”</span> <em>The Journal of Finance</em> 19 (3): 425–42. <a href="https://doi.org/10.2307/2329297">https://doi.org/10.2307/2329297</a>.
</div>
<div id="ref-Sharpe1966" class="csl-entry" role="listitem">
———. 1966. <span>“<span>Mutual Fund Performance</span>.”</span> <em>The Journal of Business</em> 39 (S1): 119. <a href="https://doi.org/10.1086/294846">https://doi.org/10.1086/294846</a>.
</div>
<div id="ref-Shen2017" class="csl-entry" role="listitem">
Shen, Lucinda. 2017. <span>“<span class="nocase">Here’s How Much the Top Hedge-Fund Manager Made Last Year</span>.”</span> <a href="http://fortune.com/2017/05/16/hedge-fund-james-simons-renaissance-technologies/">http://fortune.com/2017/05/16/hedge-fund-james-simons-renaissance-technologies/</a>.
</div>
<div id="ref-Markowitz1952" class="csl-entry" role="listitem">
Stulz, Rene M. 1995. <span>“<span class="nocase">American Finance Association, Report of the Managing Editor of the Journal of Finance for the Year 1994</span>.”</span> <em>The Journal of Finance</em> 50 (3): 1013. <a href="https://doi.org/10.2307/2329297">https://doi.org/10.2307/2329297</a>.
</div>
<div id="ref-Sutton2017" class="csl-entry" role="listitem">
Sutton, R S, and A G Barto. 2017. <span>“<span>Reinforcement Learning: An Introduction</span>.”</span> <em>Neural Networks IEEE Transactions on</em> 9 (2): 1054. <a href="https://doi.org/10.1109/TNN.1998.712192">https://doi.org/10.1109/TNN.1998.712192</a>.
</div>
<div id="ref-Turner2015" class="csl-entry" role="listitem">
Turner, Matt. 2015. <span>“<span class="nocase">The robot revolution is coming for Wall Street traders</span>.”</span> <a href="http://www.businessinsider.com/robots-to-replace-wall-street-traders-2015-8?IR=T">http://www.businessinsider.com/robots-to-replace-wall-street-traders-2015-8?IR=T</a>.
</div>
<div id="ref-Weber2012" class="csl-entry" role="listitem">
Weber, Thomas A. 2012. <span>“<span class="nocase">Price Theory in Economics</span>.”</span> In <em>The Oxford Handbook of Pricing Management</em>. <a href="https://doi.org/10.1093/oxfordhb/9780199543175.013.0017">https://doi.org/10.1093/oxfordhb/9780199543175.013.0017</a>.
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>The banking sector is frequently identified in empirical literature as a primary industry implementing blockchain technology.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button,
        { trigger: "manual",
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config);
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined;
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            }
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          }
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>
