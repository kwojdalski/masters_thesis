# TD3 Configuration with TradingEnv Backend (XAI Asset Management Library)
# ========================================================================
# This configuration uses the tradingenv library for continuous portfolio allocation
# with TD3 (Twin Delayed DDPG) algorithm
# https://github.com/xaiassetmanagement/tradingenv

experiment_name: "td3_tradingenv_sine"
seed: 42

# Data Configuration
data:
  data_path: "./data/raw/synthetic/sine_wave_ppo_no_trend.parquet"
  download_data: false
  exchange_names: ["synthetic"]
  symbols: ["SINE/WAVE"]
  timeframe: "1h"
  data_dir: "data"
  download_since: "2024-01-01T00:00:00+00:00"
  train_size: 5000  # Increased from 450 to match n_samples - prevents severe overfitting
  no_features: false

# Environment Configuration - TradingEnv Backend
env:
  name: "TRADINGENV_TD3_PORTFOLIO"
  backend: "tradingenv"

  # Transaction costs
  trading_fees: 0.001  # 0.1% proportional fee

  # Column specifications - IMPORTANT: price_columns must contain only positive values
  price_columns: ["close"]  # Columns to use as asset prices (OHLC data)
  feature_columns: ["feature_log_return", "feature_high", "feature_low"]  # Normalized features

  # Note: positions parameter is not used with tradingenv backend
  # Actions are continuous portfolio weights instead of discrete positions

  # Reward function configuration
  reward_type: "log_return"  # Options: "log_return", "differential_sharpe"
  reward_eta: 0.01  # DSR learning rate (only used when reward_type="differential_sharpe")

# Network Architecture
network:
  actor_hidden_dims: [128, 64]  # Larger network for better pattern learning (was [64, 32])
  value_hidden_dims: [128, 64]  # Larger critics for stable Q-value estimation (was [64, 32])

# Training Parameters - TD3
training:
  algorithm: "TD3"  # Twin Delayed DDPG - off-policy actor-critic

  # Learning rates - CRITICAL: Reduced for stability
  actor_lr: 0.0001  # Reduced from 0.0003 to prevent Q-value explosions
  actor_weight_decay: 0
  value_lr: 0.0001  # Reduced from 0.0003 for more conservative updates
  value_weight_decay: 0.00001

  # Training loop
  max_steps: 50000  # Increased from 10,000 for better convergence
  init_rand_steps: 2000  # CRITICAL: Increased to 4% of max_steps for better buffer diversity
  frames_per_batch: 200
  optim_steps_per_batch: 5  # CRITICAL: Prevents overfitting to small batches
  sample_size: 128  # Increased from 64 for more stable gradient estimates
  buffer_size: 50000  # Increased from 20,000 to match max_steps

  # Discount factor
  gamma: 0.99  # Discount factor for future rewards

  # TD3-specific parameters
  tau: 0.001  # CRITICAL: Slower target updates (was 0.005) for more stable targets
  policy_noise: 0.02  # CRITICAL: Further reduced (was 0.05) for cleaner learning signal
  noise_clip: 0.3  # Reduced from 0.5 - tighter noise bounds
  policy_delay: 2  # Frequency of delayed policy updates
  delay_actor: true  # Enable delayed actor updates
  delay_qvalue: true  # Enable delayed Q-value updates
  exploration_noise_std: 0.02  # CRITICAL: Reduced from 0.05 for less random exploration


  # Loss function
  loss_function: "smooth_l1"  # CRITICAL: More stable than l2 for TD learning

  # Evaluation
  eval_steps: 440
  eval_interval: 500
  log_interval: 100  # Reduced from 200 for more frequent monitoring

# Logging Configuration
logging:
  log_dir: "logs/td3_tradingenv_sine"
  log_level: "INFO"
