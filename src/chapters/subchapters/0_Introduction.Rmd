# Introduction

Trading floors are usually perceived as places with noisy people and general frenzy. This is partially justified since at some point, roughly 30 years ago, the reality was much like that. Since then the whole industry has been evolving rapidly.
Financial markets was always interested in inventions from computer science world, it is nothing new. 
Furthermore, it is part of the game - having a competitive edge in technology helps in gaining extraordinary outcomes. 
The industry was often the first one to adopt state-of-the-art technologies - increasing efficiency is in its DNA. From sophisticated 
Bloomberg terminals in the 90s to blockchain and super low latency infrastructure in 2018. [^1]

[^1]: Banking sector is one of the most often mentioned sector to adopt blockchain.

For trading entities, as rational ones, the infinite and main goal is to maximize its profits on the markets.
One can utilize many different approaches to achieve it and not all of them are essentially sophisticated. In fact, one of the most famous investors ever, Warren Buffett, still uses buy-and-hold strategy, a passive one. 

In last years, though, the most emerging category of strategies are probably the ones based on AI/machine learning paradigms. The trend is caused by improving computation power, reducing infrastructure costs and on top of it - the fact that humans tend to be more error-prone [ref]. The consensus is their impact in decision making and execution process should be assisted or even replaced by machines @Turner2015.  It is more than likely that automation of trading is going to persist over the next years and decades. It contradicts with the cliches mentioned at the beginning of this chapter.

Even though machine learning as a discipline is nothing new - the fundamentals come from the 50s, 
the trading still has not (explicitly) deployed it very widely.
For instance, in instutional forex trading, only a few biggest banks and sophisticated hedge funds have capabilities to prepare efficient machine learning-based trading systems @Mosic2017. 

In the work, the author tries to explore ways in which such machine learning (reinforcement learning)
could be developed and possibly help in gaining relatively good results.

Currently, the majority of systems described in the literature aim to maximize trading profits or risk-adjusted measures.
Although many attempts to bring a consistently profitable system have taken place, basing on variables inspired by econometrics of financial markets, fundamental analysis or machine learning, most of them were not successful due to several reasons. Among them are:

* Often large drawdowns - too large variability in results
* High transaction costs making them impractical
* High complexity - too high computation time, especially for high frequency trading.

Even if a given research gives an extraordinary result, once it goes to the public, the competitive edge is going to diminish. 
The most profitable strategies must be kept in secret. Otherwise, they are useless.

### Work Structure

The whole work is structured in such a way that a reader is able to understand the whole concept of the applied reinforcement learning algorithms.
The first part consists of the detailed introduction to the problem. It outlines
the whole concept of the AI-related fields in finance. It brought up historical background of finance and computer sciences, and its interdependency. 

The next chapter starts with the outline of selected papers from quantitative finance. 
It includes both classic models, such as CAPM, the gold standard in equity research, and modern ones. 
The part is descriptive as it regards implicit pros and cons of financial models. 
The literature review is specifically about algorithmic trading and the methodology of other similar researches.

The third part is about machine learning, it is important to come up with an explanation as to how reinforcement learning can be the best out of known machine learning categories 
and in what circumstances. Moreover, it contains comparison of most important ML groups so that a reader can distinct them easily. 
The author brought up most important concepts and aspects of RL with examples, but also potential problems, shortcomings, limitations the algorithms
are associated with.

The next part contains details of the research, such as main objectives of the work, description of the used data, 
experiment design, and finally the results. The idea was to create such agents that potentially outperform 
benchmark in risk-return measures on FX market data, agents must be robust, learn from experience, and 
deliver consistent results. The chapter describes the methodology - all formulas and steps that directed to final results. It looks at each layer of the trading system - 
#TO DO, risk management, optimization

The used algorithms are based on dynamic optimization approach. Besides value function based on Differential Sharpe Ratio, there will be several indicators, e.g. RSI, which serve as a base for decision taking of the algorithm. The methodology will include transactional costs, so that the optimization is implemented in a real-like environment

The value function will be based by several statistics, such as the Sharpe and the Differental Sharpe Ratio to capture both risk and return.
The algorithm output will be a set of the agents' actions in the form of ${-1,0,1}$.
In the last section of the chapter, the performance of the trading system (RL-based agents) is demonstrated and examined against benchmarks:

* Buy-and-hold strategy (holding long-position in selected currency pairs) 
* Random actions - this part of the algorithm will generate random values in a domain of $\{-1,0,1\}$ . These values will serve as a position in the underlying pairs. The benchmark will not include any transactional costs as this obvious that this extreme case would have an enormous cumulated transactional cost (position would change in $\frac{2}{3}$ of states).


The final part of the work outlines conclusion. It compares the results with similar works and suggests possible guidelines for research extensions. It addresses such questions as: 

* What can be additionally implemented? 
* What were limitations and what must be done to avoid them in the future?


